# 多云网络 - MultiCloud Networking Software - MCNS

几乎所有的云厂商都直接提供了自家的 L4/L7 层负载均衡服务，而且提供的 API 各有区别。
此外市场上也有若干开源或闭源的 L4/L7 层负载均衡。

那么是用开源组件自建负载均衡合适，还是应该直接使用云厂商提供的负载均衡服务，还是选择第三方厂商的负载均衡服务？
如果考虑到迁移到多云的可能性，又该如何选择负载均衡服务呢？
还是说我们应该更进一步，直接使用第三方云服务商提供的跨云网络方案，打通多个云服务商的内网？

本文的主要目的就是回答这些问题。

最理想的情况，当然是一套多云服务组件，能够完全屏蔽掉底层云服务商之间的区别，使我们的应用原生跨云，可以很方便地跨云管理、跨云调度。
但是理想的场景不一定符合当前的实际状况。

## DNS 负载均衡

许多 DNS 服务商都提供按地域的 DNS 就近解析、DNS 加权负载均衡等功能，比如 Route53，可以使用这些在 DNS 层实现多 IP 之间的负载均衡。

## L4 层负载均衡

L4 层负载均衡的配置参数很少，可替换性很高。

在公有云上，我目前认为最佳方案是直接使用各云服务商提供的 L4 层负载均衡服务（如 AWS NLB），没必要自建或者使用第三方服务。

但是某些情况下，开源方案相比自建方案确实会有些优势，比如新特性或者更低的成本，因此也可以看看如下开源方案：

- https://github.com/acassen/keepalived
  - 基于 VRRP 实现 VIP 的高可用，基于 IPVS 实现 L4 负载均衡
  - 是最常用的自建 L4 负载均衡方案，在云上自行搭建 keepalived 也是有案例可询的，值得考虑。
- https://github.com/facebookincubator/katran
  - Facebook 开源的一个基于 BPF 跟 XDP 的高性能 L4 负载均衡
- https://www.haproxy.com/blog/haproxy-on-aws-best-practices-part-3/
  - haproxy 算是比较传统的 L4 负载均衡
- https://github.com/iqiyi/dpvs
  - 爱奇艺开源的基于 DPDK 与 LVS 的 L4 负载均衡，但是项目不活跃了

在国外的云服务上设计网络架构时，需要重点考虑的一点是——**跨区流量成本**。
在四层负载均衡中，如果设计为在所有 upstream 上进行负载均衡，肯定会产生响应比例的跨区流量成本。
而如果设计为仅在各可用区内分别进行负载均衡，那就还需要在**每个可用区内单独做高可用**（K8s 的强项），并且还得具备（在部分可用区故障时）**屏蔽部分可用区的能力**（直接在 L4 负载均衡上关闭掉某个区，DNS 中去掉该区 LB 的解析）。

而对于国内的云服务，可以忽略掉跨区流量成本，只考虑高可用的能力。

## L7 层负载均衡 - API 网关

我们使用 L7 负载均衡（API 网关）的主要目的，通常是如下几个：

- 负载均衡相关能力：负载均衡、流量切分、流量镜像、限流限并发
- 监控：按 Host/Path 等纬度划分的状态码、延时指标监控
- 访问日志：统一收集好访问日志后，可使用 SQL 对全局流量的 Host/Path/Headers/Params 等进行深度分析

最简单的方案仍然是直接使用云服务商提供的 L7 层负载均衡服务（如 AWS ALB）。

但是各云服务商的 L7 层负载均衡的 API 都有较大区别，如果依赖太深，会存在被云服务商绑定的风险，多云环境下维护多套不同的负载均衡器也比较麻烦。
为了避免被云服务商绑定或者为了在多云上拥有统一的网关层使用体验，可以考虑使用开源组件自建 L7 负载均衡。

自建 L7 负载均衡的部署方案有两种，一是前端直接使用云服务商的 L4 负载均衡作为代理，相关文档：

- [Active-Active HA for NGINX Plus on AWS Using AWS Network Load Balancer](https://docs.nginx.com/nginx/deployment-guides/amazon-web-services/high-availability-network-load-balancer/)
  - Nginx 官方提供的在 AWS 上使用 AWS NLB + Nginx 的部署方案

另一个方案就是跟裸机部署一样，使用 keedalived 做高可用，相关文档：

- [Active-Passive HA for NGINX Plus on AWS Using Elastic IP Addresses](https://docs.nginx.com/nginx/deployment-guides/amazon-web-services/high-availability-keepalived/)
  - Nginx 官方也提供的在 AWS 上使用 keepalived + Nginx 的部署方案

你可以在 Nginx 官方文档中看到其他云服务商的部署方案。

### API 网关选型

>https://landscape.cncf.io/card-mode?category=api-gateway&grouping=category

开源的 Nginx 在很长一段时间内都是 L7 负载均衡领域的王者，但是随着云计算的发展，企业对 API 网关的需求越来越强烈，开源 Nginx 本身渐渐无法满足需求。

最大的问题是 Nginx 的配置比较复杂，而且不够动态。而新一代 API 网关则瞄准了这一需求，提供基于 API 的动态配置方式，内置网关常用的所有能力，赢得了许多用户的喜爱，下面先列举一下当前市面上有点名气的 API 网关。


基于 Envoy 实现的 API 网关：

- Istio IngressGateway
  - 我司当前使用的方案
- https://github.com/projectcontour/contour
- https://github.com/emissary-ingress/emissary

基于 Ngninx/Openresty 实现的 API 网关：

- Kong：国外老牌项目，部分关键插件是收费的。
- APISIX：国产项目，完全开源，借鉴了 Kong 的方案，但是官方宣称性能更好，架构更优。

其他 API 网关：

- github.com/megaease/easegress
- https://github.com/apioak/apioak
- https://github.com/luraproject/lura
- https://github.com/TykTechnologies/tyk
- https://github.com/alibaba/Sentinel

接下来对上述网关做一个对比分析，考虑下应该选择哪个，又该如何落地。

### Kubernetes 多集群网关架构

#### 1. Istio Multi-cluster

对于多集群位于同一网络的情况，集群的 Pods 间可以直接通讯，这正是我们想要的。

而对于多集群位于不同网络的情况，集群的 Pods 间可以通过 Istio 的 Gateway 来通讯，这也很合适。
但我目前认为位于不同网络的情况更多是指跨云（多云/混合云），这种情况直接把多集群网络打通可能不太好，更合适的方案，可能是在外部单独设里一个网关或者直接通过 DNS 来处理不同云之间的差异。

因此这里我们只讨论多集群位于同一网络内的情况，我主要关注如下几点：

- 控制面与网关层如何做到高可用？
  - 控制面跟网关单独用两个集群部署？这样即使挂了一个控制面集群，另一个仍然能工作。而且这样成本也不高，就多几个 istiod 的 pods  而已。
- 多集群的 Istio 升级时要如何处理？
- 能否实现将服务间的通讯限制在同一可用区避免跨区流量？
- 如何实现为服务任选多个集群进行部署？
  - 使用 karmada 可能是个比较好的方式

#### 1. Solo Mesh

>https://www.solo.io/products/gloo-mesh/

Gloo Mesh 是一个兼容 Istio 的集群网关方案，它的架构实际上可以简单解释为「双层网关」，示意图如下：

![](_img/gloo-mesh-multi-cluster.svg)

![](_img/gloo-multi-cluster-api-gateway.webp)

这样做在单集群网络中存在的缺点是，双层网关之间可能会造成跨区流量，而且也会导致双倍的网关成本。主要就看跨区流量有没有优化方案，以及双倍网关成本能否接受。

但是 gloo-mesh 这项目去年底开始就一直没人维护过，前途堪忧：

- https://github.com/solo-io/gloo-mesh

#### 2. Google Multi-cluster Gateway

- [The Kubernetes Gateway API: Multi-cluster Gateways - Google - 2021](https://www.youtube.com/watch?v=NGsHFi276NE)
  - [GCP 提供多集群网关](https://cloud.google.com/kubernetes-engine/docs/how-to/deploying-multi-cluster-gateways)的能力，多个集群的 Pod 可以共享一个 Gateway

Google Multi-cluster Gateway 是一个单层 Gateway 的方案，这使它性能更高，而且相比双层 Gateway 它的跨区流量比例会更小，此外它直接使用了 Kubernetes Gateway API 来实现多集群网关的配置，这提高了配置的可移植性。

![](_img/google-multi-cluster-gateway.svg)

但是它也有明显的缺点：

- 当前仅适用于 Google Cloud
- Gateway API 目前处于 v1alpha2 阶段，GKE 不为此功能提供任何 SLA 保证或技术支持。
  - 此外在 Gateway API 升级到 v1beta1 之前会进行重大 API 更改，现在直接使用此功能显然不是个很好的选择。

#### 3. karmada

- https://github.com/karmada-io/karmada

karmada 跟 Google 闭源的 Multi-cluster Gateway 一样，都是基于 Kubernetes Federation 衍生出来的方案，使用方法基本一致。

但是 karmada 默认只提供跨集群 Service 能力，新补充的 MultiClusterIngress 感觉还不太稳定，建议再观望观望。

此外 karmada 的 Ingress 方案有可能会导致跨区流量。

#### 3. kubevela cluster gateway

- https://github.com/oam-dev/cluster-gateway

思路上看着是单层网关，部署在单独的 K8s 集群，但是 stars 数还太少，可以研究下，但是稳定性或许难以保证。


## 多云网关架构

在单云内，网络是互通的，因此总体来看只需要一个 L7 边缘网关就能处理所有的流量。

而多云场景下，如果多云之间是完全等价的，那就只需要在 DNS 上完成多云之间的负载均衡就行。

但是通常而言，所云之间并不完全等价，这时可能就需要一个多云之间的「全局 L7 网关」，负责处理流量在多云之间的分配。

## 多云网络市场分析

针对多云网络市场，目前也有多家厂商在这个领域竞争，当前（2022-04）主要有如下几家（按字母顺序排列）：

- [Alkira Cloud Networking](https://www.alkira.com/cloud-networking/)
  - Single and multi-cloud networking should be as simple as any other cloud service.
- [Arrcus Multi-Cloud Networking](https://arrcus.com/solutions/multi-cloud/)
  - onsisting of ArcEdge, as a secure data plane software and ArcOrchestrator dramatically shortens multi-cloud networking set up time from days to hours. 
  - 使用 Terraform 自动化部署多云网络
- [Arista Data-Driven Cloud Networking](https://www.arista.com/en/solutions/cloud-networking)
  - an open approach to cloud networking around a single consistent software platform, the Arista EOS® network stack, and network data lake architecture (NetDL™), with the application of artificial intelligence and machine learning (AI/ML) to automation and security challenges.
- [Aviatrix Cloud Networking Platform](https://aviatrix.com/cloud-network-platform/#multi-cloud-architecture/)
  - brings multi-cloud networking, security, and operational visibility capabilities that go beyond what any cloud service provider offers. Aviatrix software leverages public cloud provider APIs to interact with and directly program native cloud networking constructs, abstracting the unique complexities of each cloud to form one network data plane.
  - 使用 Terraform/Ansible 自动化部署多云网络
- [Cohesive VNS3](https://www.cohesive.net/)
  - Encrypted Overlay Network
  - Whether using a single cloud service provider, multiple clouds, or hybrid cloud, the VNS3 Network Platform provides the connectivity and security you need with better pricing, better support, and better visibility.
- [Cisco Cloud ACI](https://www.cisco.com/c/en/us/solutions/data-center-virtualization/application-centric-infrastructure/cloud-aci.html)
  - Automated network connectivity, consistent policy management, and simplified ops for multicloud.
- [F5 Distributed Cloud Platform](https://www.f5.com/cloud/products/platform-overview)
  - delivers improved functionality, advanced security controls, and more simplified operations than native services from cloud providers.
- [Prosimo](https://prosimo.io/)
  - One platform that integrates cloud networking, performance, security, observability, driven by data
- [VMware Avi](https://avinetworks.com/)
  - The Multi-Cloud Application Services Platform
- [VMware NSX Cloud](https://www.vmware.com/products/nsx-cloud.html)
  - Deliver consistent networking and security for your applications running natively in public clouds with NSX Cloud.

其中有的产品会使用 overlay 网络完全屏蔽各云服务商底层的网络差异，但是代价是性能更差；也有的产品直接编排云服务商原生的网络能力实现跨云。

MCNS 提供所有核心网络能力，底层能力如路由，高级能力如 L4/L7 服务（内容分发网络 CDN，ADC、防火墙），以及其他关键的能力集成，比如 terraform/pulumi/ansible 等云环境配置管理与自动化运维工具。如下图所示：

![](_img/MCNS_Features.png)

驱动用户选择多云网络方案而不是直接使用各云服务商原生网络的原因在于，直接使用原生网络方案存在如下痛点：

- 配置太过繁琐：仅仅为了做一点微小的改动，比如说改下 ACL 规则，我就需要打开不同云服务商的 28 个浏览器页面，并且做一堆的配置变更。
- 带宽限制：通过 VPN 直接连接多云，通常会受限于 VPN 的带宽
- 统一网络堆栈：希望能够在统一的面板上管理所有云上的网络资源
- 高级网络路由：大部分云服务商提供的专有网络 VPC，都不支持 BGP 协议
- 各云服务商的功能、设计思路不一致，导致多云环境的运维、故障排查困难。

其他多云网络方案：

- [多云管理与 CloudFlare](https://www.cloudflare.com/zh-cn/learning/cloud/what-is-multicloud-management/)
- [F5’s Multi-Cloud Networking Strategy and the Gartner 2021 Cool Vendors in Cloud Computing Report](https://www.f5.com/company/blog/multi-cloud-gartner-cool-vendor-cloud-computing-report)
- [Market Guide for Multicloud Networking Software](https://aviatrix.com/gartner-multicloud-networking-software-market-guide/)
  - [Multicloud Networking Software (MCNS)](https://blogs.gartner.com/andrew-lerner/2022/04/21/multicloud-networking-software-mcns/)


总的来说，多云网络目前还处于早期发展阶段，overlay 的网络彻底屏蔽了各云服务商的网络差异，而 underlay 网络方案的配置复杂性很难避免。
目前还是先观望观望吧，不急于作出选择，毕竟我目前接触到的场景，还没复杂到必须选择一个多云网络架构，当前的架构优化优化，缝缝补补又能顶三年 emmmm
